# conf/config.yaml
defaults:
  - _self_

num_train_epochs: 3
batch_size: 48
accumulation_steps: 16
n_head_layers: 4

logging_steps: 100
save_steps: 500
eval_steps: 500
enc_mlm_prob: 0.30
dec_mlm_prob: 0.50
dec_mlm_overlap: "inclusive"  # Options: random, inclusive, exclusive
mlm_enc_loss_weight: 1.0
mlm_dec_loss_weight: 1.0

wandb: True
wandb_project: "modernbert-lexmae"
log_every: 20
model:
  model_name_or_path: "answerdotai/modernbert-base"
  max_length: 512

checkpoint:
  checkpoint_path: "checkpoints/modernbert-lexmae"
  max_to_keep: 3
  checkpoint_every: 500
optimizer:
  learning_rate: 5e-4
  enc_learning_rate: 3e-5
  dec_learning_rate: 8e-4
  warmup_steps: 15000
  every_k_schedule: 4
  weight_decay: 0.01
